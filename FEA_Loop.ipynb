{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b0bed5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "midway3-0277.rcc.local\n"
     ]
    }
   ],
   "source": [
    "!hostname"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dce1bff",
   "metadata": {},
   "source": [
    "# Imports/Setpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# import os\n",
    "# import shutil\n",
    "\n",
    "# username = os.environ.get('USER')\n",
    "# scratch_base = f\"/scratch/midway3/{username}/fea_project\"\n",
    "# custom_lib_path = os.path.join(scratch_base, \"my_custom_libs\")\n",
    "# py_version = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "# site_packages = os.path.join(custom_lib_path, \"lib\", py_version, \"site-packages\")\n",
    "\n",
    "# # Clean up old installation\n",
    "# if os.path.exists(custom_lib_path):\n",
    "#     print(f\"Removing old libraries...\")\n",
    "#     shutil.rmtree(custom_lib_path)\n",
    "\n",
    "# install_cmd = (\n",
    "#     f\"pip install --target='{site_packages}' \"\n",
    "#     \"--upgrade \"\n",
    "#     \"--no-cache-dir \"\n",
    "#     \"'numpy<2.0' \"\n",
    "#     \"'pandas' \"\n",
    "#     \"'bottleneck' \"\n",
    "#     \"'numexpr' \"\n",
    "#     \"'torch' \"\n",
    "#     \"'torchvision' \"\n",
    "#     \"'accelerate>=0.31.0' \"\n",
    "#     \"'tensorflow' \"\n",
    "#     \"'tf-keras' \"\n",
    "#     \"'transformers' \"\n",
    "#     \"'sentence-transformers' \"\n",
    "#     \"'datasets' \"\n",
    "#     \"'botocore' \"\n",
    "#     \"'aiobotocore' \"\n",
    "#     \"'s3fs' \"\n",
    "#     \"'openpyxl' \"\n",
    "#     \"'sentencepiece' \"\n",
    "#     \"'plotly' \"\n",
    "#     \"'scikit-learn' \"\n",
    "#     \"'papermill' \"\n",
    "#     \"'scrapbook' \"\n",
    "#     \"'openai' \"\n",
    "#     \"'pydantic' \"\n",
    "#     \"'tqdm' \"\n",
    "# )\n",
    "\n",
    "# print(\"Installing with NumPy <2.0...\")\n",
    "# exit_code = os.system(install_cmd)\n",
    "\n",
    "# if exit_code == 0:\n",
    "#     print(\"\\nSUCCESS: Complete stack installed.\")\n",
    "# else:\n",
    "#     print(f\"\\nERROR: Installation failed with code {exit_code}\")\n",
    "\n",
    "# print(\"\\n##### RESTART KERNEL NOW #####\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13787109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python is looking here first: /scratch/midway3/None/fea_project\\my_custom_libs\\lib\\python3.13\\site-packages\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     25\u001b[39m os.environ[\u001b[33m'\u001b[39m\u001b[33mPYTHONUSERBASE\u001b[39m\u001b[33m'\u001b[39m] = custom_lib_path\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# --- 3. NOW IMPORT LIBRARIES ---\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Only import AFTER sys.path is updated\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m \n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msentence_transformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\datasets\\__init__.py:17\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[32m     15\u001b[39m __version__ = \u001b[33m\"\u001b[39m\u001b[33m4.5.0\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\datasets\\arrow_dataset.py:104\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfilesystems\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_remote_filesystem\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfingerprint\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     93\u001b[39m     fingerprint_transform,\n\u001b[32m     94\u001b[39m     format_kwargs_for_fingerprint,\n\u001b[32m   (...)\u001b[39m\u001b[32m    102\u001b[39m     validate_fingerprint,\n\u001b[32m    103\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m format_table, get_format_type_from_alias, get_formatter, query_table\n\u001b[32m    105\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformatting\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformatting\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LazyDict, _is_range_contiguous\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01minfo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DatasetInfo, DatasetInfosDict\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\datasets\\formatting\\__init__.py:91\u001b[39m\n\u001b[32m     88\u001b[39m     _register_unavailable_formatter(_polars_error, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m, aliases=[\u001b[33m\"\u001b[39m\u001b[33mpl\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config.TORCH_AVAILABLE:\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtorch_formatter\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TorchFormatter\n\u001b[32m     93\u001b[39m     _register_formatter(TorchFormatter, \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m, aliases=[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpytorch\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\datasets\\formatting\\torch_formatter.py:32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Import torch once at module level once\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     34\u001b[39m     _torch_available = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\__init__.py:2247\u001b[39m\n\u001b[32m   2231\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   2232\u001b[39m \u001b[38;5;66;03m# Import most common subpackages\u001b[39;00m\n\u001b[32m   2233\u001b[39m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2238\u001b[39m \n\u001b[32m   2239\u001b[39m \u001b[38;5;66;03m# needs to be before import torch.nn as nn to avoid circular dependencies\u001b[39;00m\n\u001b[32m   2240\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mautograd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[32m   2241\u001b[39m     enable_grad \u001b[38;5;28;01mas\u001b[39;00m enable_grad,\n\u001b[32m   2242\u001b[39m     inference_mode \u001b[38;5;28;01mas\u001b[39;00m inference_mode,\n\u001b[32m   2243\u001b[39m     no_grad \u001b[38;5;28;01mas\u001b[39;00m no_grad,\n\u001b[32m   2244\u001b[39m     set_grad_enabled \u001b[38;5;28;01mas\u001b[39;00m set_grad_enabled,\n\u001b[32m   2245\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2247\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m   2248\u001b[39m     __config__ \u001b[38;5;28;01mas\u001b[39;00m __config__,\n\u001b[32m   2249\u001b[39m     __future__ \u001b[38;5;28;01mas\u001b[39;00m __future__,\n\u001b[32m   2250\u001b[39m     _awaits \u001b[38;5;28;01mas\u001b[39;00m _awaits,\n\u001b[32m   2251\u001b[39m     accelerator \u001b[38;5;28;01mas\u001b[39;00m accelerator,\n\u001b[32m   2252\u001b[39m     autograd \u001b[38;5;28;01mas\u001b[39;00m autograd,\n\u001b[32m   2253\u001b[39m     backends \u001b[38;5;28;01mas\u001b[39;00m backends,\n\u001b[32m   2254\u001b[39m     cpu \u001b[38;5;28;01mas\u001b[39;00m cpu,\n\u001b[32m   2255\u001b[39m     cuda \u001b[38;5;28;01mas\u001b[39;00m cuda,\n\u001b[32m   2256\u001b[39m     distributed \u001b[38;5;28;01mas\u001b[39;00m distributed,\n\u001b[32m   2257\u001b[39m     distributions \u001b[38;5;28;01mas\u001b[39;00m distributions,\n\u001b[32m   2258\u001b[39m     fft \u001b[38;5;28;01mas\u001b[39;00m fft,\n\u001b[32m   2259\u001b[39m     futures \u001b[38;5;28;01mas\u001b[39;00m futures,\n\u001b[32m   2260\u001b[39m     hub \u001b[38;5;28;01mas\u001b[39;00m hub,\n\u001b[32m   2261\u001b[39m     jit \u001b[38;5;28;01mas\u001b[39;00m jit,\n\u001b[32m   2262\u001b[39m     linalg \u001b[38;5;28;01mas\u001b[39;00m linalg,\n\u001b[32m   2263\u001b[39m     mps \u001b[38;5;28;01mas\u001b[39;00m mps,\n\u001b[32m   2264\u001b[39m     mtia \u001b[38;5;28;01mas\u001b[39;00m mtia,\n\u001b[32m   2265\u001b[39m     multiprocessing \u001b[38;5;28;01mas\u001b[39;00m multiprocessing,\n\u001b[32m   2266\u001b[39m     nested \u001b[38;5;28;01mas\u001b[39;00m nested,\n\u001b[32m   2267\u001b[39m     nn \u001b[38;5;28;01mas\u001b[39;00m nn,\n\u001b[32m   2268\u001b[39m     optim \u001b[38;5;28;01mas\u001b[39;00m optim,\n\u001b[32m   2269\u001b[39m     overrides \u001b[38;5;28;01mas\u001b[39;00m overrides,\n\u001b[32m   2270\u001b[39m     profiler \u001b[38;5;28;01mas\u001b[39;00m profiler,\n\u001b[32m   2271\u001b[39m     sparse \u001b[38;5;28;01mas\u001b[39;00m sparse,\n\u001b[32m   2272\u001b[39m     special \u001b[38;5;28;01mas\u001b[39;00m special,\n\u001b[32m   2273\u001b[39m     testing \u001b[38;5;28;01mas\u001b[39;00m testing,\n\u001b[32m   2274\u001b[39m     types \u001b[38;5;28;01mas\u001b[39;00m types,\n\u001b[32m   2275\u001b[39m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[32m   2276\u001b[39m     version \u001b[38;5;28;01mas\u001b[39;00m version,\n\u001b[32m   2277\u001b[39m     xpu \u001b[38;5;28;01mas\u001b[39;00m xpu,\n\u001b[32m   2278\u001b[39m )\n\u001b[32m   2279\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msignal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m windows \u001b[38;5;28;01mas\u001b[39;00m windows\n\u001b[32m   2282\u001b[39m \u001b[38;5;66;03m# Quantized, sparse, AO, etc. should be last to get imported, as nothing\u001b[39;00m\n\u001b[32m   2283\u001b[39m \u001b[38;5;66;03m# is expected to depend on them.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\nested\\__init__.py:21\u001b[39m\n\u001b[32m     11\u001b[39m __all__ = [\n\u001b[32m     12\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mto_padded_tensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mas_nested_tensor\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmasked_select\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     18\u001b[39m ]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Allowlist these for weights_only load of NJT\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _rebuild_njt, NestedTensor \u001b[38;5;28;01mas\u001b[39;00m _NestedTensor\n\u001b[32m     24\u001b[39m torch.serialization.add_safe_globals([_NestedTensor, _rebuild_njt])\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mas_nested_tensor\u001b[39m(\n\u001b[32m     28\u001b[39m     ts: Tensor | \u001b[38;5;28mlist\u001b[39m[Tensor] | \u001b[38;5;28mtuple\u001b[39m[Tensor, ...],\n\u001b[32m     29\u001b[39m     dtype: DType | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     30\u001b[39m     device: Device | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     31\u001b[39m     layout=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     32\u001b[39m ) -> Tensor:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\nested\\_internal\\nested_tensor.py:7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_C\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DispatchKey, DispatchKeySet\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_prims_common\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_expandable_to\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_internal\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnested_int\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NestedIntNode\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mweak\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WeakTensorKeyDictionary\n\u001b[32m     11\u001b[39m _tensor_id_counter = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\nested\\_internal\\nested_int.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_constant_symnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConstantIntNode\n\u001b[32m      7\u001b[39m __all__ = [\u001b[33m\"\u001b[39m\u001b[33mNestedIntNode\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Python version of aten/src/ATen/core/NestedIntSymNodeImpl.cpp\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\fx\\__init__.py:88\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33mr\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03mFX is a toolkit for developers to use to transform ``nn.Module``\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03minstances. FX consists of three main components: a **symbolic tracer,**\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03mrepository.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m immutable_collections\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_symbolic_trace\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     89\u001b[39m     PH,\n\u001b[32m     90\u001b[39m     ProxyableClassMeta,\n\u001b[32m     91\u001b[39m     symbolic_trace,\n\u001b[32m     92\u001b[39m     Tracer,\n\u001b[32m     93\u001b[39m     wrap,\n\u001b[32m     94\u001b[39m )\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CodeGen, Graph  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\fx\\_symbolic_trace.py:24\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopaque_object\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_opaque_reference_type, is_opaque_type\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lazy_graph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _make_graph_module\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _PyTreeCodeGen, _PyTreeInfo, Graph\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GraphModule\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\fx\\_lazy_graph_module.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcontextlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m contextmanager\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfx\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph_module\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m      5\u001b[39m     _format_import_block,\n\u001b[32m      6\u001b[39m     GraphModule,\n\u001b[32m      7\u001b[39m     reduce_graph_module,\n\u001b[32m      8\u001b[39m     reduce_package_graph_module,\n\u001b[32m      9\u001b[39m )\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpackage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PackageExporter, sys_importer\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\fx\\graph_module.py:24\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexperimental\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _config \u001b[38;5;28;01mas\u001b[39;00m fx_experimental_config\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgraph\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     25\u001b[39m     _BoxedCodeGen,\n\u001b[32m     26\u001b[39m     _custom_builtins,\n\u001b[32m     27\u001b[39m     _is_from_torch,\n\u001b[32m     28\u001b[39m     _override_sym_repr,\n\u001b[32m     29\u001b[39m     _PyTreeCodeGen,\n\u001b[32m     30\u001b[39m     Graph,\n\u001b[32m     31\u001b[39m     PythonCode,\n\u001b[32m     32\u001b[39m )\n\u001b[32m     35\u001b[39m __all__ = [\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreduce_graph_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreduce_package_graph_module\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     38\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mGraphModule\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m ]\n\u001b[32m     41\u001b[39m _USER_PRESERVED_ATTRIBUTES_KEY = \u001b[33m\"\u001b[39m\u001b[33m_user_preserved_attributes\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\aesteva\\Downloads\\python-3.13.12-embed-amd64\\Lib\\site-packages\\torch\\fx\\graph.py:29\u001b[39m\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_library\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mopaque_object\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_opaque_value_type\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_dtype_abbrs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dtype_abbrs\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _pytree \u001b[38;5;28;01mas\u001b[39;00m fx_pytree\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_compatibility\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compatibility\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimmutable_collections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m immutable_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1360\u001b[39m, in \u001b[36m_find_and_load\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1322\u001b[39m, in \u001b[36m_find_and_load_unlocked\u001b[39m\u001b[34m(name, import_)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1262\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(name, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1556\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1530\u001b[39m, in \u001b[36m_get_spec\u001b[39m\u001b[34m(cls, fullname, path, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:1629\u001b[39m, in \u001b[36mfind_spec\u001b[39m\u001b[34m(self, fullname, target)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap_external>:152\u001b[39m, in \u001b[36m_path_stat\u001b[39m\u001b[34m(path)\u001b[39m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "### Installs all the packages in the install library (my_custom_libs) ##\n",
    "\n",
    "# --- 1. SETUP PATHS FIRST (you are going to need to change to whatever your path to the instal library is in) ---\n",
    "username = os.environ.get('USER')\n",
    "scratch_base = f\"/scratch/midway3/{username}/fea_project\"\n",
    "custom_lib_path = os.path.join(scratch_base, \"my_custom_libs\")\n",
    "hf_cache_path = os.path.join(scratch_base, \"hf_cache\")\n",
    "\n",
    "py_version = f\"python{sys.version_info.major}.{sys.version_info.minor}\"\n",
    "site_packages = os.path.join(custom_lib_path, \"lib\", py_version, \"site-packages\")\n",
    "sys.path.insert(0, site_packages)\n",
    "\n",
    "# FORCE custom path to the FRONT of the list\n",
    "if site_packages not in sys.path:\n",
    "    sys.path.insert(0, site_packages)\n",
    "\n",
    "print(f\"Python is looking here first: {sys.path[0]}\")\n",
    "\n",
    "# --- 2. ENV VARIABLES ---\n",
    "os.environ['HF_HOME'] = hf_cache_path\n",
    "os.environ['TRANSFORMERS_CACHE'] = hf_cache_path\n",
    "os.environ['PYTHONUSERBASE'] = custom_lib_path\n",
    "\n",
    "# --- 3. NOW IMPORT LIBRARIES ---\n",
    "# Only import AFTER sys.path is updated\n",
    "import datasets \n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- 4. VERIFY PATHS ---\n",
    "print(f\"Datasets is loaded from: {os.path.dirname(datasets.__file__)}\")\n",
    "\n",
    "if \"software\" in os.path.dirname(datasets.__file__):\n",
    "    print(\"FAILURE: Still loading system datasets.\")\n",
    "else:\n",
    "    print(\"SUCCESS: All libraries isolated in scratch.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ec22fa46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "    print(f\"Added current directory to sys.path: {current_dir}\")\n",
    "import papermill as pm\n",
    "import scrapbook as sb\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import importlib\n",
    "import free_entailments_algorithm_utils as fea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc0f9e",
   "metadata": {},
   "source": [
    "# Global Variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8706c167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change to False if running the real thing.\n",
    "\n",
    "test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a317584c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "importlib.reload(fea)\n",
    "\n",
    "#List of all sentences/sentence ids being used\n",
    "df_p = pd.read_excel(\"ClauseLevel_df_p.xlsx\")\n",
    "df_sc = pd.read_excel(\"ClauseLevel_df_sc.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "afd352ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Generating premise-premise pairs ===\n",
      "  Dataset size: 49155 (9564 books, 39591 speeches)\n",
      "  Max possible valid pairs: 424,378,590\n",
      "  Seen-index tracking: 0 BB + 0 BS already consumed (45,730,266 BB + 378,648,324 BS remaining)\n",
      "  Sampling 762,755 pairs from 424,378,590 possible...\n",
      "    BB: sampled 82,193 pairs (seen 0, avail 45,730,266)\n",
      "    BS: sampled 680,562 pairs (seen 0, avail 378,648,324)\n",
      "  Generated 762,755 unique pairs\n",
      "\n",
      "=== Generating conclusion-conclusion pairs ===\n",
      "  Dataset size: 15289 (2511 books, 12778 speeches)\n",
      "  Max possible valid pairs: 35,236,863\n",
      "  Seen-index tracking: 0 BB + 0 BS already consumed (3,151,305 BB + 32,085,558 BS remaining)\n",
      "  Sampling 237,245 pairs from 35,236,863 possible...\n",
      "    BB: sampled 21,217 pairs (seen 0, avail 3,151,305)\n",
      "    BS: sampled 216,028 pairs (seen 0, avail 32,085,558)\n",
      "  Generated 237,245 unique pairs\n",
      "\n",
      "=== SUMMARY ===\n",
      "Premise-premise pairs: 762,755\n",
      "Conclusion-conclusion pairs: 237,245\n",
      "Total valid pairs: 1,000,000\n",
      "Columns: ['id1', 'id2']\n",
      "Filtered out 0 already-labeled pairs (kept 1,000,000 of 1,000,000)\n",
      "Generated 1,000,000 unlabeled pairs (after excluding already-labeled)\n"
     ]
    }
   ],
   "source": [
    "## All clauses. Of the form: argument_id\tsentence_id\tsentence\n",
    "\n",
    "df_clause = pd.concat([df_p,df_sc]).drop_duplicates(subset='sentence_id')\n",
    "\n",
    "# Generate initial unlabeled pairs (production mode only).\n",
    "# We generate up to 50M pairs (BB + BS), excluding any already-labeled pairs.\n",
    "# These are randomly sampled from the ~450M valid candidate space.\n",
    "# seen_indices_dir persists consumed pair indices across rounds so\n",
    "# subsequent rounds never re-sample pairs from earlier rounds.\n",
    "\n",
    "if not test:\n",
    "    df_pairs = fea.generate_valid_pairs(\n",
    "        df_p,\n",
    "        df_sc,\n",
    "        'sentence_id',\n",
    "        'sentence',\n",
    "        max_pairs=1_000_000,\n",
    "        exclude_labeled_csv=\"labeled_pairs/llm_labeled_pairs.csv\",\n",
    "        seen_indices_dir=\"seen_pair_indices\",\n",
    "    )\n",
    "    print(f\"Generated {len(df_pairs):,} unlabeled pairs (after excluding already-labeled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a7c4e388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Creates Embedding Cache (If not already created) from df_clause:\n",
    "# import pickle\n",
    "\n",
    "# cache_path = \"embedding_cache_finetuned.pkl\"\n",
    "\n",
    "# if os.path.exists(cache_path):\n",
    "#     with open(cache_path, 'rb') as f:\n",
    "#         embedding_cache_finetuned = pickle.load(f)\n",
    "#     print(f\"Loaded existing embedding cache: {len(embedding_cache_finetuned)} embeddings\")\n",
    "# else:\n",
    "#     # Create cache from all unique clauses\n",
    "#     embedding_cache_finetuned = fea.create_embedding_cache(\n",
    "#         df_texts=df_clause,\n",
    "#         id_col='sentence_id',\n",
    "#         text_col='sentence',\n",
    "#         model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "#         batch_size=128,\n",
    "#         show_progress_bar=True\n",
    "#     )\n",
    "#     # Save to disk for future runs\n",
    "#     with open(cache_path, 'wb') as f:\n",
    "#         pickle.dump(embedding_cache_finetuned, f)\n",
    "#     print(f\"Saved embedding cache to {cache_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4affa1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set difference: 1,000,000 - 10 = 1,000,000 rows\n",
      "Unlabeled pairs after removing labeled: 1,000,000\n"
     ]
    }
   ],
   "source": [
    "input_file = \"labeled_pairs/Results_DS_BtoS_iteration_0.csv\"\n",
    "df_input = pd.read_csv(input_file)\n",
    "\n",
    "if test:\n",
    "    importlib.reload(fea)\n",
    "    df_input = pd.read_csv(input_file)\n",
    "    \n",
    "    input_file, remaining_llm_calls = fea.two_random_subsamples(\n",
    "        df = df_input,\n",
    "        frac1 = 0.8,\n",
    "        frac2 = 0.2,\n",
    "        random_state = 42\n",
    "    )\n",
    "else:\n",
    "    remaining_llm_calls = None\n",
    "    # Use Results_DS_BtoS_iteration_0 as labeled input,\n",
    "    # and subtract already-labeled pairs from the 20M unlabeled pool\n",
    "    df_entailed_pairs = df_input.rename(columns={'sentence_id_1': 'id1', 'sentence_id_2': 'id2'})\n",
    "    unlabeled_pairs = fea.setminus(df_pairs, df_entailed_pairs, [\"id1\", \"id2\"])\n",
    "    print(f\"Unlabeled pairs after removing labeled: {len(unlabeled_pairs):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "812698fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded embedding cache: 63909 embeddings\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"embedding_cache_finetuned.pkl\", 'rb') as f:\n",
    "    embedding_cache_finetuned = pickle.load(f)\n",
    "print(f\"Loaded embedding cache: {len(embedding_cache_finetuned)} embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f463c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 3          # hard cap (set high – budget is the real limiter)\n",
    "iteration_stats = []\n",
    "\n",
    "sent_frac = 0.5\n",
    "budget = 0.0\n",
    "\n",
    "# ---- Budget controls ----\n",
    "# The loop stops automatically once cumulative LLM spend reaches budget_dollars.\n",
    "# Set to None for unlimited (loop runs until num_iterations).\n",
    "budget_dollars = 0.10       \n",
    "cost_per_1k_pairs = 2.0       # ~$2 per 1,000 pairs (DeepSeek R1)\n",
    "\n",
    "# DeepSeek API key for production LLM calls\n",
    "api_key = os.environ.get(\"DEEPSEEK_API_KEY\", \"sk-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a607ca8",
   "metadata": {},
   "source": [
    "# FEA Loop using Papermill\n",
    "\n",
    "This notebook uses papermill to iteratively run the FEA_Pipeline notebook, feeding the output of each iteration as input to the next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f52a5cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"fea_iterations\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "start_iteration = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5393ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting FEA Loop with 3 iterations\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting FEA Loop with {num_iterations} iterations\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "21b4bd7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "VALIDATING INITIAL DATA STATE\n",
      "============================================================\n",
      "Initial labeled pairs: 10\n",
      "Initial unlabeled pairs: 1000000\n",
      "Total: 1000010\n",
      "✓ Cleaned up old loop data directory\n",
      "Starting with 10 already labeled pairs\n",
      "Unlabeled pool: 1000000\n",
      "Total pairs: 1000010\n",
      "Budget: $0.10  ($2.00 / 1k pairs)\n",
      "\n",
      "============================================================\n",
      "ITERATION 0/3\n",
      "============================================================\n",
      "Status: Labeled=10, Unlabeled=1000000, Total=1000010\n",
      "Cost so far: $0.00  ($0.10 remaining)\n",
      "Executing FEA_Pipeline.ipynb with input: labeled_pairs/Results_DS_BtoS_iteration_0.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8fba55dbefd405f8bc56a9fe894ba3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/26 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Retrieved df_to_llm from scrapbook\n",
      "  Shape: (50, 7)\n",
      "  Columns: ['sentence_id_2', 'sentence_id_1', 'sentence_text_2', 'argument_id_2', 'sentence_text_1', 'argument_id_1', 'score']\n",
      "✓ Saved df_to_llm to fea_iterations\\loop_data/df_to_llm_iter_0.csv\n",
      "\n",
      "============================================================\n",
      "EXECUTING LLM EVALUATOR (forward direction)\n",
      "============================================================\n",
      "Sending 50 pairs to LLM...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8d3881e41746299155edb0415a2e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Executing:   0%|          | 0/12 [00:00<?, ?cell/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Forward LLM evaluation complete\n",
      "✓ LLM labeled pairs updated: 62 total in labeled_pairs/llm_labeled_pairs.csv\n",
      "\n",
      "============================================================\n",
      "ONE-WAY RESULTS PROCESSING\n",
      "============================================================\n",
      "Total input pairs: 50\n",
      "Resolved rows: 56 (YES=0, NO=56)\n",
      "  ↳ Inferred reverse NO (money saved): 28\n",
      "Need reverse LLM call: 22\n",
      "============================================================\n",
      "\n",
      "✓ LLM labeled pairs updated: 90 total in labeled_pairs/llm_labeled_pairs.csv\n",
      "✓ Recorded 28 inferred reverse-NO pairs in labeled_pairs/llm_labeled_pairs.csv\n",
      "\n",
      "============================================================\n",
      "SENDING 22 REVERSE PAIRS TO LLM\n",
      "============================================================\n",
      "Model: deepseek-reasoner\n",
      "Input: labeled_pairs\\reverse_iter_0_input.csv\n",
      "Output: labeled_pairs\\reverse_iter_0_output.csv\n",
      "Loading data from labeled_pairs\\reverse_iter_0_input.csv...\n",
      "Loading data from ArgLevel_ClauseIds_df.xlsx...\n",
      "Loaded 22 sentence pairs\n",
      "Using model: deepseek-reasoner\n",
      "Using prompt type: test_prompt_tot_json2\n",
      "Running batch evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [00:00<00:00, 47.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] content length: 1505, reasoning_content length: 5191\n",
      "[DEBUG] content preview: {\n",
      "  \"sentence_id_1\": \"B0258001sc\",\n",
      "  \"sentence_id_2\": \"B0126001sc\",\n",
      "  \"answers\": \"YES, YES, YES\",\n",
      "  \"reasoning\": \"1. Logically, if sovereign power requires the consent of the people (Statement 1), then any legitimate holder of that power—i.e., rulers—must derive their legitimacy from that same conse\n",
      "[DEBUG] reasoning preview: We are given two statements derived from two arguments. Statement 1: \"Sovereign power requires the consent of the people.\" Statement 2: \"Rulers derive their legitimacy from the consent of the governed.\" We need to assess whether Statement 1 entails Statement 2. Entailment means that if Statement 1 i\n",
      "Saving progress at batch 1...\n",
      "Saving progress at batch 2...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_1.csv\n",
      "Saving progress at batch 3...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_2.csv\n",
      "Saving progress at batch 4...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_3.csv\n",
      "Saving progress at batch 5...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_4.csv\n",
      "Saving progress at batch 6...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_5.csv\n",
      "Saving progress at batch 7...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_6.csv\n",
      "Saving progress at batch 8...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_7.csv\n",
      "Saving progress at batch 9...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_8.csv\n",
      "Saving progress at batch 10...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_9.csv\n",
      "Saving progress at batch 11...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_10.csv\n",
      "Saving progress at batch 12...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_11.csv\n",
      "Saving progress at batch 13...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_12.csv\n",
      "Saving progress at batch 14...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_13.csv\n",
      "Saving progress at batch 15...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_14.csv\n",
      "Saving progress at batch 16...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_15.csv\n",
      "Saving progress at batch 17...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_16.csv\n",
      "Saving progress at batch 18...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_17.csv\n",
      "Saving progress at batch 19...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_18.csv\n",
      "Saving progress at batch 20...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_19.csv\n",
      "Saving progress at batch 21...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_20.csv\n",
      "Saving progress at batch 22...\n",
      "Deleting previous file: labeled_pairs\\reverse_iter_0_output_progress_batch_21.csv\n",
      "Saved results to labeled_pairs\\reverse_iter_0_output.csv\n",
      "✓ Reverse LLM evaluation complete\n",
      "✓ LLM labeled pairs updated: 112 total in labeled_pairs/llm_labeled_pairs.csv\n",
      "\n",
      "============================================================\n",
      "REVERSE RESULTS SUMMARY\n",
      "============================================================\n",
      "Total reverse pairs resolved: 44\n",
      "  Bidirectional YES: 16\n",
      "  Bidirectional NO:  28\n",
      "============================================================\n",
      "\n",
      "\n",
      "============================================================\n",
      "BIDIRECTIONAL PROCESSING COMPLETE\n",
      "============================================================\n",
      "Total resolved pairs: 100\n",
      "  YES (bidirectional): 16\n",
      "  NO:                  84\n",
      "Saved to: labeled_pairs/Results_DS_BtoS_iteration_1.csv\n",
      "============================================================\n",
      "\n",
      "✓ Merged 10 previous + 100 new = 110 total\n",
      "✓ Processed 50 NEW pairs\n",
      "  → Labeled input: 10\n",
      "  → Unlabeled available: 1000000\n",
      "  → Iteration cost: $0.10  (cumulative: $0.10)\n",
      "\n",
      "============================================================\n",
      "BUDGET EXHAUSTED  ($0.10 / $0.10)\n",
      "Stopping before iteration 1.\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FEA Loop Complete!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(fea)\n",
    "\n",
    "iteration_stats = fea.run_fea_loop(\n",
    "    test=test,\n",
    "    input_file=input_file,\n",
    "    df_clause=df_clause,\n",
    "    embedding_cache=embedding_cache_finetuned,\n",
    "    num_iterations=num_iterations,\n",
    "    start_iteration=start_iteration,\n",
    "    output_dir=output_dir,\n",
    "    sent_frac=sent_frac,\n",
    "    budget=budget,\n",
    "    remaining_llm_calls=remaining_llm_calls,\n",
    "    unlabeled_pairs=unlabeled_pairs if 'unlabeled_pairs' in dir() else None,\n",
    "    deepseek_api_key=api_key,\n",
    "    budget_dollars=budget_dollars,\n",
    "    cost_per_1k_pairs=cost_per_1k_pairs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ecc7483-2853-472e-ba44-4c50a17ff30c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration Statistics:\n",
      "   iteration  labeled_input  unlabeled_available  pairs_selected  \\\n",
      "0          0              6                 1000               1   \n",
      "1          1              7                  999               1   \n",
      "\n",
      "   cumulative_labeled  unlabeled_remaining  \\\n",
      "0                   7                  999   \n",
      "1                   8                  998   \n",
      "\n",
      "                                     output_file  entailment_ratio  \\\n",
      "0  labeled_pairs/Results_DS_BtoS_iteration_1.csv               0.0   \n",
      "1  labeled_pairs/Results_DS_BtoS_iteration_2.csv               0.0   \n",
      "\n",
      "   iteration_cost  cumulative_cost  \n",
      "0           0.002            0.002  \n",
      "1           0.002            0.004  \n",
      "\n",
      "============================================================\n",
      "COST SUMMARY\n",
      "============================================================\n",
      "Total pairs sent to LLM: 2\n",
      "Total cost: $0.00\n",
      "============================================================\n",
      "\n",
      "✓ Saved iteration statistics to fea_iterations\\iteration_stats.csv\n"
     ]
    }
   ],
   "source": [
    "stats_df = fea.save_iteration_stats(iteration_stats, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bec4fd-a9ce-43ce-935b-38d81fa52d49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
