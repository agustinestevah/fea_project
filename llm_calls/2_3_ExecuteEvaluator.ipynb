{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3d07de-6634-4a9a-a518-8f87c8dd3150",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055bc81-f312-4699-bda8-c6ac10a5f596",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c13416dd-1bf8-441e-be6c-78343fec8621",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openai'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydantic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ValidationError\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (accuracy_score, f1_score,\n\u001b[32m     17\u001b[39m                              precision_score, recall_score)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openai'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pydantic import ValidationError\n",
    "from sklearn.metrics import (accuracy_score, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import llm_calls.deepseek_evaluator as etb\n",
    "from llm_calls.deepseek_evaluator import EntailmentEvaluator\n",
    "from llm_calls.prompts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec256c-801f-4182-87da-90c5af1bf98b",
   "metadata": {},
   "source": [
    "## Declare paths "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4520a-4220-4e8f-b717-edb94511448b",
   "metadata": {},
   "source": [
    "# Execute Entailment API calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1905db28-a96b-45fb-b745-a5905efa596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check API key and environment\n",
    "print(\"Validating environment setup...\")\n",
    "\n",
    "# Check for OpenAI/DeepSeek API key\n",
    "api_key = os.environ.get('OPENAI_API_KEY') or os.environ.get('DEEPSEEK_API_KEY')\n",
    "if api_key:\n",
    "    print(f\"✓ API key found (length: {len(api_key)} chars)\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: No API key found in environment variables\")\n",
    "    print(\"  Looking for OPENAI_API_KEY or DEEPSEEK_API_KEY\")\n",
    "    print(\"  LLM API calls will likely fail without proper credentials\")\n",
    "\n",
    "# Check if deepseek_evaluator module is importable\n",
    "try:\n",
    "    import llm_calls.deepseek_evaluator as etb\n",
    "    print(f\"✓ deepseek_evaluator module loaded from: {etb.__file__}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR: Cannot import deepseek_evaluator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check OpenAI library\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    print(f\"✓ OpenAI library available\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ WARNING: OpenAI library issue: {e}\")\n",
    "\n",
    "print(\"\\nEnvironment validation complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16702ad1-0eda-448c-903d-50e781839013",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-reasoner\"\n",
    "input_file = \"fea_iterations/loop_data/df_to_llm_iter_0.csv\"\n",
    "args_file = \"ArgLevel_ClauseIds_df.xlsx\"\n",
    "prompt = \"test_prompt_tot_json2\"\n",
    "output = \"labeled_pairs/Results_DS_BtoS_iteration_1.csv\"\n",
    "previous_input_file = \"Results_DS_BtoS_iteration_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71fe9c0-0a7f-4f90-8352-a0714d68a028",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Validate input files exist\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INPUT FILE VALIDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"Checking input_file: {input_file}\")\n",
    "if os.path.exists(input_file):\n",
    "    df_input = pd.read_csv(input_file)\n",
    "    print(f\"✓ Input file found: {len(df_input)} rows\")\n",
    "    print(f\"  Columns: {list(df_input.columns)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "print(f\"\\nChecking args_file: {args_file}\")\n",
    "if os.path.exists(args_file):\n",
    "    if args_file.endswith('.csv'):\n",
    "        df_args = pd.read_csv(args_file)\n",
    "    elif args_file.endswith('.xlsx'):\n",
    "        df_args = pd.read_excel(args_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {args_file}\")\n",
    "    print(f\"✓ Args file found: {len(df_args)} rows\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Args file not found: {args_file}\")\n",
    "\n",
    "if previous_input_file:\n",
    "    print(f\"\\nChecking previous_input_file: {previous_input_file}\")\n",
    "    if os.path.exists(previous_input_file):\n",
    "        df_prev = pd.read_csv(previous_input_file)\n",
    "        print(f\"✓ Previous input file found: {len(df_prev)} rows\")\n",
    "    else:\n",
    "        print(f\"⚠ WARNING: Previous input file not found: {previous_input_file}\")\n",
    "        print(f\"  Continuing without merging previous results\")\n",
    "\n",
    "# Create output directory if needed\n",
    "output_dir = os.path.dirname(output)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\n✓ Created output directory: {output_dir}\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfd4470-be9f-48ac-b28c-a0dcde930a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip .csv extension since evaluator adds it automatically\n",
    "output_arg = output.replace('.csv', '') if output.endswith('.csv') else output\n",
    "\n",
    "sys.argv = [\n",
    "    \"deepseek_evaluator.py\",\n",
    "    \"--model\", llm_model,\n",
    "    \"--file\", input_file,\n",
    "    \"--external\", args_file,\n",
    "    \"--prompt\", prompt,\n",
    "    \"--output\", output_arg\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXECUTING LLM API CALLS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {llm_model}\")\n",
    "print(f\"Input: {input_file}\")\n",
    "print(f\"Output (arg): {output_arg}\")\n",
    "print(f\"Expected file: {output}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "try:\n",
    "    etb.main()\n",
    "    print(f\"\\n✓ LLM API calls completed\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR during LLM API execution: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Check for output file - evaluator adds .csv extension\n",
    "if not os.path.exists(output):\n",
    "    # Check if double extension was created\n",
    "    double_ext = output + '.csv' if not output.endswith('.csv') else output.replace('.csv', '') + '.csv.csv'\n",
    "    if os.path.exists(double_ext):\n",
    "        print(f\"⚠ WARNING: Output created with double extension: {double_ext}\")\n",
    "        print(f\"  Renaming to: {output}\")\n",
    "        os.rename(double_ext, output)\n",
    "    else:\n",
    "        # List files in output directory for debugging\n",
    "        output_dir = os.path.dirname(output) or '.'\n",
    "        print(f\"⚠ Files in {output_dir}:\")\n",
    "        for f in os.listdir(output_dir):\n",
    "            print(f\"  - {f}\")\n",
    "        raise FileNotFoundError(f\"Output file was not created: {output}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VALIDATING OUTPUT\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_new = pd.read_csv(output)\n",
    "print(f\"✓ Output file loaded: {len(df_new)} rows\")\n",
    "\n",
    "# Check for required columns\n",
    "required_cols = ['sentence_id_1', 'sentence_id_2']\n",
    "missing_cols = [col for col in required_cols if col not in df_new.columns]\n",
    "if missing_cols:\n",
    "    print(f\"⚠ WARNING: Missing expected columns: {missing_cols}\")\n",
    "    print(f\"  Available columns: {list(df_new.columns)}\")\n",
    "\n",
    "# Check for errors in the data\n",
    "if 'ERROR' in df_new.columns:\n",
    "    error_count = df_new['ERROR'].notna().sum()\n",
    "    if error_count > 0:\n",
    "        print(f\"⚠ WARNING: {error_count}/{len(df_new)} rows contain errors\")\n",
    "        print(f\"  First error: {df_new[df_new['ERROR'].notna()]['ERROR'].iloc[0]}\")\n",
    "\n",
    "# Check for empty/null critical fields\n",
    "null_counts = df_new[required_cols].isnull().sum()\n",
    "if null_counts.any():\n",
    "    print(f\"⚠ WARNING: Null values found in critical columns:\")\n",
    "    for col, count in null_counts[null_counts > 0].items():\n",
    "        print(f\"  {col}: {count}/{len(df_new)} rows\")\n",
    "\n",
    "print(f\"✓ Output validation complete\\n\")\n",
    "\n",
    "# Merge with previous results\n",
    "if previous_input_file and os.path.exists(previous_input_file):\n",
    "    print(f\"{'='*60}\")\n",
    "    print(\"MERGING WITH PREVIOUS RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    df_previous = pd.read_csv(previous_input_file)\n",
    "    print(f\"✓ Loaded previous results: {len(df_previous)} rows\")\n",
    "    \n",
    "    df_merged = pd.concat([df_previous, df_new], ignore_index=True)\n",
    "    print(f\"✓ Merged: {len(df_previous)} previous + {len(df_new)} new = {len(df_merged)} total\")\n",
    "    \n",
    "    # Validate merged data\n",
    "    if len(df_merged) != len(df_previous) + len(df_new):\n",
    "        print(f\"⚠ WARNING: Merged row count doesn't match expected sum\")\n",
    "    \n",
    "    df_merged.to_csv(output, index=False)\n",
    "    print(f\"✓ Saved merged results to {output}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "else:\n",
    "    print(f\"\\n✓ No previous input file to merge\")\n",
    "    print(f\"✓ Output saved to {output}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a34fe8-ee9e-40af-828b-76e6fe7ff140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_final = pd.read_csv(output)\n",
    "print(f\"Total rows in final output: {len(df_final)}\")\n",
    "\n",
    "if 'llm_conclusion_12' in df_final.columns:\n",
    "    conclusion_counts = df_final['llm_conclusion_12'].value_counts()\n",
    "    print(f\"\\nLLM Conclusions:\")\n",
    "    for conclusion, count in conclusion_counts.items():\n",
    "        print(f\"  {conclusion}: {count} ({count/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for failed calls\n",
    "    failed = df_final['llm_conclusion_12'].isnull().sum()\n",
    "    if failed > 0:\n",
    "        print(f\"\\n⚠ Failed API calls: {failed}/{len(df_final)} ({failed/len(df_final)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Column 'llm_conclusion_12' not found in output\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ Pipeline execution complete\")\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
