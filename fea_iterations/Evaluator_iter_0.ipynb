{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f3d07de-6634-4a9a-a518-8f87c8dd3150",
   "metadata": {
    "papermill": {
     "duration": 0.004703,
     "end_time": "2026-02-13T09:13:11.259774",
     "exception": false,
     "start_time": "2026-02-13T09:13:11.255071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9055bc81-f312-4699-bda8-c6ac10a5f596",
   "metadata": {
    "papermill": {
     "duration": 0.004962,
     "end_time": "2026-02-13T09:13:11.269190",
     "exception": false,
     "start_time": "2026-02-13T09:13:11.264228",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c13416dd-1bf8-441e-be6c-78343fec8621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:11.283021Z",
     "iopub.status.busy": "2026-02-13T09:13:11.282662Z",
     "iopub.status.idle": "2026-02-13T09:13:16.135058Z",
     "shell.execute_reply": "2026-02-13T09:13:16.133375Z"
    },
    "papermill": {
     "duration": 4.861003,
     "end_time": "2026-02-13T09:13:16.137393",
     "exception": false,
     "start_time": "2026-02-13T09:13:11.276390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import importlib\n",
    "import json\n",
    "import os\n",
    "import statistics\n",
    "import sys\n",
    "import time\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "from pydantic import ValidationError\n",
    "from sklearn.metrics import (accuracy_score, f1_score,\n",
    "                             precision_score, recall_score)\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure project root is on sys.path (required for papermill fresh kernels)\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "import llm_calls.deepseek_evaluator as etb\n",
    "from llm_calls.deepseek_evaluator import EntailmentEvaluator\n",
    "from llm_calls.prompts import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec256c-801f-4182-87da-90c5af1bf98b",
   "metadata": {
    "papermill": {
     "duration": 0.007411,
     "end_time": "2026-02-13T09:13:16.154585",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.147174",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Declare paths "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4520a-4220-4e8f-b717-edb94511448b",
   "metadata": {
    "papermill": {
     "duration": 0.009864,
     "end_time": "2026-02-13T09:13:16.174039",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.164175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Execute Entailment API calls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1905db28-a96b-45fb-b745-a5905efa596b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:16.196035Z",
     "iopub.status.busy": "2026-02-13T09:13:16.194916Z",
     "iopub.status.idle": "2026-02-13T09:13:16.211561Z",
     "shell.execute_reply": "2026-02-13T09:13:16.209006Z"
    },
    "papermill": {
     "duration": 0.031294,
     "end_time": "2026-02-13T09:13:16.213798",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.182504",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validating environment setup...\n",
      "✓ API key found (length: 35 chars)\n",
      "✓ deepseek_evaluator module loaded from: c:\\Users\\aesteva\\Documents\\GitHub\\fea_project\\llm_calls\\deepseek_evaluator.py\n",
      "✓ OpenAI library available\n",
      "\n",
      "Environment validation complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check API key and environment\n",
    "print(\"Validating environment setup...\")\n",
    "\n",
    "# Check for OpenAI/DeepSeek API key\n",
    "api_key = os.environ.get('OPENAI_API_KEY') or os.environ.get('DEEPSEEK_API_KEY')\n",
    "if api_key:\n",
    "    print(f\"✓ API key found (length: {len(api_key)} chars)\")\n",
    "else:\n",
    "    print(\"⚠ WARNING: No API key found in environment variables\")\n",
    "    print(\"  Looking for OPENAI_API_KEY or DEEPSEEK_API_KEY\")\n",
    "    print(\"  LLM API calls will likely fail without proper credentials\")\n",
    "\n",
    "# Check if deepseek_evaluator module is importable\n",
    "try:\n",
    "    import llm_calls.deepseek_evaluator as etb\n",
    "    print(f\"✓ deepseek_evaluator module loaded from: {etb.__file__}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ ERROR: Cannot import deepseek_evaluator: {e}\")\n",
    "    raise\n",
    "\n",
    "# Check OpenAI library\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    print(f\"✓ OpenAI library available\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ WARNING: OpenAI library issue: {e}\")\n",
    "\n",
    "print(\"\\nEnvironment validation complete.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16702ad1-0eda-448c-903d-50e781839013",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:16.239629Z",
     "iopub.status.busy": "2026-02-13T09:13:16.238683Z",
     "iopub.status.idle": "2026-02-13T09:13:16.248978Z",
     "shell.execute_reply": "2026-02-13T09:13:16.246433Z"
    },
    "papermill": {
     "duration": 0.027074,
     "end_time": "2026-02-13T09:13:16.251312",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.224238",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "llm_model = \"deepseek-reasoner\"\n",
    "input_file = \"fea_iterations/loop_data/df_to_llm_iter_0.csv\"\n",
    "args_file = \"ArgLevel_ClauseIds_df.xlsx\"\n",
    "prompt = \"test_prompt_tot_json2\"\n",
    "output = \"labeled_pairs/Results_DS_BtoS_iteration_1.csv\"\n",
    "previous_input_file = \"Results_DS_BtoS_iteration_0.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "453b02fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:16.278750Z",
     "iopub.status.busy": "2026-02-13T09:13:16.277493Z",
     "iopub.status.idle": "2026-02-13T09:13:16.287959Z",
     "shell.execute_reply": "2026-02-13T09:13:16.285443Z"
    },
    "papermill": {
     "duration": 0.026298,
     "end_time": "2026-02-13T09:13:16.290184",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.263886",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "llm_model = \"deepseek-reasoner\"\n",
    "input_file = \"fea_iterations\\\\loop_data/df_to_llm_iter_0.csv\"\n",
    "args_file = \"ArgLevel_ClauseIds_df.xlsx\"\n",
    "prompt = \"test_prompt_tot_json2\"\n",
    "output = \"labeled_pairs/Results_DS_BtoS_iteration_1_one_way\"\n",
    "previous_input_file = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71fe9c0-0a7f-4f90-8352-a0714d68a028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:16.316511Z",
     "iopub.status.busy": "2026-02-13T09:13:16.315654Z",
     "iopub.status.idle": "2026-02-13T09:13:21.719423Z",
     "shell.execute_reply": "2026-02-13T09:13:21.718463Z"
    },
    "papermill": {
     "duration": 5.420413,
     "end_time": "2026-02-13T09:13:21.721303",
     "exception": false,
     "start_time": "2026-02-13T09:13:16.300890",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INPUT FILE VALIDATION\n",
      "============================================================\n",
      "Checking input_file: fea_iterations\\loop_data/df_to_llm_iter_0.csv\n",
      "✓ Input file found: 1 rows\n",
      "  Columns: ['sentence_id_2', 'sentence_id_1', 'sentence_text_2', 'argument_id_2', 'sentence_text_1', 'argument_id_1', 'score']\n",
      "\n",
      "Checking args_file: ArgLevel_ClauseIds_df.xlsx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Args file found: 6181 rows\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Validate input files exist\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"INPUT FILE VALIDATION\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(f\"Checking input_file: {input_file}\")\n",
    "if os.path.exists(input_file):\n",
    "    df_input = pd.read_csv(input_file)\n",
    "    print(f\"✓ Input file found: {len(df_input)} rows\")\n",
    "    print(f\"  Columns: {list(df_input.columns)}\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Input file not found: {input_file}\")\n",
    "\n",
    "print(f\"\\nChecking args_file: {args_file}\")\n",
    "if os.path.exists(args_file):\n",
    "    if args_file.endswith('.csv'):\n",
    "        df_args = pd.read_csv(args_file)\n",
    "    elif args_file.endswith('.xlsx'):\n",
    "        df_args = pd.read_excel(args_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported file format: {args_file}\")\n",
    "    print(f\"✓ Args file found: {len(df_args)} rows\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Args file not found: {args_file}\")\n",
    "\n",
    "if previous_input_file:\n",
    "    print(f\"\\nChecking previous_input_file: {previous_input_file}\")\n",
    "    if os.path.exists(previous_input_file):\n",
    "        df_prev = pd.read_csv(previous_input_file)\n",
    "        print(f\"✓ Previous input file found: {len(df_prev)} rows\")\n",
    "    else:\n",
    "        print(f\"⚠ WARNING: Previous input file not found: {previous_input_file}\")\n",
    "        print(f\"  Continuing without merging previous results\")\n",
    "\n",
    "# Create output directory if needed\n",
    "output_dir = os.path.dirname(output)\n",
    "if output_dir and not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    print(f\"\\n✓ Created output directory: {output_dir}\")\n",
    "\n",
    "print(f\"{'='*60}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3cfd4470-be9f-48ac-b28c-a0dcde930a09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:13:21.737255Z",
     "iopub.status.busy": "2026-02-13T09:13:21.736916Z",
     "iopub.status.idle": "2026-02-13T09:14:44.823652Z",
     "shell.execute_reply": "2026-02-13T09:14:44.822376Z"
    },
    "papermill": {
     "duration": 83.099456,
     "end_time": "2026-02-13T09:14:44.827405",
     "exception": false,
     "start_time": "2026-02-13T09:13:21.727949",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXECUTING LLM API CALLS\n",
      "============================================================\n",
      "Model: deepseek-reasoner\n",
      "Input: fea_iterations\\loop_data/df_to_llm_iter_0.csv\n",
      "Output (arg): labeled_pairs/Results_DS_BtoS_iteration_1_one_way\n",
      "Expected file: labeled_pairs/Results_DS_BtoS_iteration_1_one_way\n",
      "============================================================\n",
      "\n",
      "Loading data from fea_iterations\\loop_data/df_to_llm_iter_0.csv...\n",
      "Loading data from ArgLevel_ClauseIds_df.xlsx...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 sentence pairs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model: deepseek-reasoner\n",
      "Using prompt type: test_prompt_tot_json2\n",
      "Running batch evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 1/1 [00:00<00:00, 47.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DEBUG] content length: 919, reasoning_content length: 10431\n",
      "[DEBUG] content preview: {\n",
      "  \"sentence_id_1\": \"B0653002sc\",\n",
      "  \"sentence_id_2\": \"S0053530002sc\",\n",
      "  \"answers\": \"YES, NO, NO\",\n",
      "  \"reasoning\": \"1. (YES) In a representative democracy, parliament is the primary institution for expressing consent; thus, if consent is essential, parliament must accurately represent the will of the\n",
      "[DEBUG] reasoning preview: We are asked to assess whether Statement 1 entails Statement 2. Statement 1: \"Consent of the governed is essential for government effectiveness.\" Statement 2: \"Parliament must accurately represent the will of the people.\"\n",
      "\n",
      "We are given Argument 1 (from Juan de Mariana, 1598) and its context. Argumen\n",
      "Saving progress at batch 1...\n",
      "Saved results to labeled_pairs/Results_DS_BtoS_iteration_1_one_way.csv\n",
      "\n",
      "✓ LLM API calls completed\n",
      "✓ Output found at labeled_pairs/Results_DS_BtoS_iteration_1_one_way.csv\n",
      "\n",
      "============================================================\n",
      "VALIDATING OUTPUT\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Strip .csv extension since evaluator adds it automatically\n",
    "output_arg = output.replace('.csv', '') if output.endswith('.csv') else output\n",
    "\n",
    "sys.argv = [\n",
    "    \"deepseek_evaluator.py\",\n",
    "    \"--model\", llm_model,\n",
    "    \"--file\", input_file,\n",
    "    \"--external\", args_file,\n",
    "    \"--prompt\", prompt,\n",
    "    \"--output\", output_arg\n",
    "]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXECUTING LLM API CALLS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Model: {llm_model}\")\n",
    "print(f\"Input: {input_file}\")\n",
    "print(f\"Output (arg): {output_arg}\")\n",
    "print(f\"Expected file: {output}\")\n",
    "print(f\"{'='*60}\\n\")\n",
    "\n",
    "try:\n",
    "    etb.main()\n",
    "    print(f\"\\n✓ LLM API calls completed\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n✗ ERROR during LLM API execution: {e}\")\n",
    "    print(f\"Error type: {type(e).__name__}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "# Resolve actual output path — evaluator adds .csv to the base name.\n",
    "# Do NOT rename/move files — just find the correct path.\n",
    "if os.path.exists(output):\n",
    "    output_actual = output\n",
    "elif os.path.exists(f\"{output}.csv\"):\n",
    "    output_actual = f\"{output}.csv\"\n",
    "    print(f\"✓ Output found at {output_actual}\")\n",
    "elif output.endswith('.csv') and os.path.exists(output.replace('.csv', '') + '.csv'):\n",
    "    output_actual = output\n",
    "else:\n",
    "    output_dir_check = os.path.dirname(output) or '.'\n",
    "    print(f\"⚠ Files in {output_dir_check}:\")\n",
    "    for f in os.listdir(output_dir_check):\n",
    "        print(f\"  - {f}\")\n",
    "    raise FileNotFoundError(f\"Output file was not created: {output}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"VALIDATING OUTPUT\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1a34fe8-ee9e-40af-828b-76e6fe7ff140",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-13T09:14:44.840199Z",
     "iopub.status.busy": "2026-02-13T09:14:44.839745Z",
     "iopub.status.idle": "2026-02-13T09:14:44.854639Z",
     "shell.execute_reply": "2026-02-13T09:14:44.853683Z"
    },
    "papermill": {
     "duration": 0.022183,
     "end_time": "2026-02-13T09:14:44.855665",
     "exception": false,
     "start_time": "2026-02-13T09:14:44.833482",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "EXECUTION SUMMARY\n",
      "============================================================\n",
      "Total rows in final output: 1\n",
      "Output file: labeled_pairs/Results_DS_BtoS_iteration_1_one_way.csv\n",
      "\n",
      "LLM Conclusions:\n",
      "  NO: 1 (100.0%)\n",
      "\n",
      "============================================================\n",
      "✓ Pipeline execution complete\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Summary Report\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"EXECUTION SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df_final = pd.read_csv(output_actual)\n",
    "print(f\"Total rows in final output: {len(df_final)}\")\n",
    "print(f\"Output file: {output_actual}\")\n",
    "\n",
    "if 'llm_conclusion_12' in df_final.columns:\n",
    "    conclusion_counts = df_final['llm_conclusion_12'].value_counts()\n",
    "    print(f\"\\nLLM Conclusions:\")\n",
    "    for conclusion, count in conclusion_counts.items():\n",
    "        print(f\"  {conclusion}: {count} ({count/len(df_final)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for failed calls\n",
    "    failed = df_final['llm_conclusion_12'].isnull().sum()\n",
    "    if failed > 0:\n",
    "        print(f\"\\n⚠ Failed API calls: {failed}/{len(df_final)} ({failed/len(df_final)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\n⚠ Column 'llm_conclusion_12' not found in output\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"✓ Pipeline execution complete\")\n",
    "print(f\"{'='*60}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 97.596079,
   "end_time": "2026-02-13T09:14:45.544078",
   "environment_variables": {},
   "exception": null,
   "input_path": "llm_calls/2_3_ExecuteEvaluator.ipynb",
   "output_path": "fea_iterations\\Evaluator_iter_0.ipynb",
   "parameters": {
    "args_file": "ArgLevel_ClauseIds_df.xlsx",
    "input_file": "fea_iterations\\loop_data/df_to_llm_iter_0.csv",
    "llm_model": "deepseek-reasoner",
    "output": "labeled_pairs/Results_DS_BtoS_iteration_1_one_way",
    "previous_input_file": "",
    "prompt": "test_prompt_tot_json2"
   },
   "start_time": "2026-02-13T09:13:07.947999",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}