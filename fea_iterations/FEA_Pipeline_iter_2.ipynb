{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5201f8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:49:49.187101Z",
     "iopub.status.busy": "2026-02-28T00:49:49.186518Z",
     "iopub.status.idle": "2026-02-28T00:50:26.096717Z",
     "shell.execute_reply": "2026-02-28T00:50:26.095015Z"
    },
    "papermill": {
     "duration": 36.921007,
     "end_time": "2026-02-28T00:50:26.098325",
     "exception": false,
     "start_time": "2026-02-28T00:49:49.177318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import scrapbook as sb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Ensure project root is on sys.path (required for papermill fresh kernels)\n",
    "current_dir = os.getcwd()\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)\n",
    "\n",
    "import free_entailments_algorithm_utils as fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80e5bba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.115342Z",
     "iopub.status.busy": "2026-02-28T00:50:26.114785Z",
     "iopub.status.idle": "2026-02-28T00:50:26.120206Z",
     "shell.execute_reply": "2026-02-28T00:50:26.119015Z"
    },
    "papermill": {
     "duration": 0.016622,
     "end_time": "2026-02-28T00:50:26.121510",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.104888",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "iteration_number = 1\n",
    "input_csv_path = \"labeled_pairs/Results_DS_BtoS_iteration_0.csv\"\n",
    "df_clause_path = None\n",
    "embedding_cache_path = None\n",
    "test = True\n",
    "remaining_llm_calls_path = None\n",
    "unlabeled_pairs_path = None\n",
    "sent_frac = 0.5\n",
    "budget = 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a8bf088",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.139795Z",
     "iopub.status.busy": "2026-02-28T00:50:26.139074Z",
     "iopub.status.idle": "2026-02-28T00:50:26.147605Z",
     "shell.execute_reply": "2026-02-28T00:50:26.145556Z"
    },
    "papermill": {
     "duration": 0.022029,
     "end_time": "2026-02-28T00:50:26.149197",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.127168",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "iteration_number = 2\n",
    "input_csv_path = \"labeled_pairs/Results_DS_BtoS_iteration_2.csv\"\n",
    "df_clause_path = \"fea_iterations\\\\loop_data/df_clause.pkl\"\n",
    "embedding_cache_path = \"fea_iterations\\\\loop_data/embedding_cache.pkl\"\n",
    "test = False\n",
    "remaining_llm_calls_path = None\n",
    "unlabeled_pairs_path = \"fea_iterations\\\\loop_data/unlabeled_pairs.pkl\"\n",
    "sent_frac = 0.5\n",
    "budget = 5.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f095ee19-2788-4f9e-8045-317fc74c3e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.176576Z",
     "iopub.status.busy": "2026-02-28T00:50:26.176198Z",
     "iopub.status.idle": "2026-02-28T00:50:26.423477Z",
     "shell.execute_reply": "2026-02-28T00:50:26.422434Z"
    },
    "papermill": {
     "duration": 0.264422,
     "end_time": "2026-02-28T00:50:26.424893",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.160471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PARAMETER VALUES AFTER PAPERMILL INJECTION:\n",
      "================================================================================\n",
      "iteration_number = 2\n",
      "test = False\n",
      "remaining_llm_calls_path = None\n",
      "df_clause_path = fea_iterations\\loop_data/df_clause.pkl\n",
      "================================================================================\n",
      "\n",
      "✓ Loaded df_clause: 38635 rows\n",
      "✓ Loaded embedding cache: 38635 embeddings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded unlabeled_pairs: 4997977 rows\n",
      "✓ All data loaded from pickle files\n"
     ]
    }
   ],
   "source": [
    "pipeline_data = fea.load_pipeline_data(\n",
    "    df_clause_path=df_clause_path,\n",
    "    embedding_cache_path=embedding_cache_path,\n",
    "    test=test,\n",
    "    remaining_llm_calls_path=remaining_llm_calls_path,\n",
    "    unlabeled_pairs_path=unlabeled_pairs_path,\n",
    "    iteration_number=iteration_number,\n",
    ")\n",
    "\n",
    "df_clause = pipeline_data['df_clause']\n",
    "embedding_cache_finetuned = pipeline_data['embedding_cache']\n",
    "remaining_llm_calls = pipeline_data['remaining_llm_calls']\n",
    "unlabeled_pairs = pipeline_data['unlabeled_pairs']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48afa33",
   "metadata": {
    "papermill": {
     "duration": 0.008166,
     "end_time": "2026-02-28T00:50:26.440554",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.432388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task 1: Seting up dataframes and Running FEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9241ee2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.454972Z",
     "iopub.status.busy": "2026-02-28T00:50:26.454669Z",
     "iopub.status.idle": "2026-02-28T00:50:26.572155Z",
     "shell.execute_reply": "2026-02-28T00:50:26.570885Z"
    },
    "papermill": {
     "duration": 0.125883,
     "end_time": "2026-02-28T00:50:26.573290",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.447407",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing 'verdict' column (630 YES, 5340 NO)\n"
     ]
    }
   ],
   "source": [
    "df_llm_original = pd.read_csv(input_csv_path)\n",
    "\n",
    "# If 'verdict' column already exists (e.g. from process_llm_results_bidirectional),\n",
    "# use it directly instead of recomputing via add_verdict (which only sees one-way\n",
    "# conclusions and would overwrite correct bidirectional verdicts).\n",
    "if 'verdict' in df_llm_original.columns and df_llm_original['verdict'].notna().any():\n",
    "    df_llm = df_llm_original\n",
    "    print(f\"Using existing 'verdict' column ({(df_llm['verdict']=='YES').sum()} YES, {(df_llm['verdict']=='NO').sum()} NO)\")\n",
    "else:\n",
    "    df_llm = fea.add_verdict(\n",
    "        df_llm_original,\n",
    "        id1_col='sentence_id_1',\n",
    "        id2_col='sentence_id_2',\n",
    "        conclusion_col='llm_conclusion_12',\n",
    "        positive_label='YES'\n",
    "    )\n",
    "\n",
    "if test:\n",
    "    df_llm_remaining = fea.add_verdict(\n",
    "        remaining_llm_calls,\n",
    "        id1_col='sentence_id_1',\n",
    "        id2_col='sentence_id_2',\n",
    "        conclusion_col='llm_conclusion_12',\n",
    "        positive_label='YES'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ec35c84",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.591426Z",
     "iopub.status.busy": "2026-02-28T00:50:26.591130Z",
     "iopub.status.idle": "2026-02-28T00:50:26.649555Z",
     "shell.execute_reply": "2026-02-28T00:50:26.648213Z"
    },
    "papermill": {
     "duration": 0.069441,
     "end_time": "2026-02-28T00:50:26.650646",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.581205",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered 18 pairs (kept 5952).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0137002p</td>\n",
       "      <td>S0022948006p</td>\n",
       "      <td>Legitimate authority derives from the consent ...</td>\n",
       "      <td>The populace is united in their desire for a s...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>S0022948006p</td>\n",
       "      <td>B0137002p</td>\n",
       "      <td>The populace is united in their desire for a s...</td>\n",
       "      <td>Legitimate authority derives from the consent ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0691012p</td>\n",
       "      <td>S0023235007p</td>\n",
       "      <td>Prioritizing the people's welfare is essential...</td>\n",
       "      <td>The Commons is tasked with protecting the righ...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>S0023235007p</td>\n",
       "      <td>B0691012p</td>\n",
       "      <td>The Commons is tasked with protecting the righ...</td>\n",
       "      <td>Prioritizing the people's welfare is essential...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0360002p</td>\n",
       "      <td>S0023525004p</td>\n",
       "      <td>When those in power, such as kings and royal o...</td>\n",
       "      <td>I have fulfilled my duty to my nation by speak...</td>\n",
       "      <td>NO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id1           id2  \\\n",
       "0     B0137002p  S0022948006p   \n",
       "1  S0022948006p     B0137002p   \n",
       "2     B0691012p  S0023235007p   \n",
       "3  S0023235007p     B0691012p   \n",
       "4     B0360002p  S0023525004p   \n",
       "\n",
       "                                               text1  \\\n",
       "0  Legitimate authority derives from the consent ...   \n",
       "1  The populace is united in their desire for a s...   \n",
       "2  Prioritizing the people's welfare is essential...   \n",
       "3  The Commons is tasked with protecting the righ...   \n",
       "4  When those in power, such as kings and royal o...   \n",
       "\n",
       "                                               text2 verdict  \n",
       "0  The populace is united in their desire for a s...      NO  \n",
       "1  Legitimate authority derives from the consent ...      NO  \n",
       "2  The Commons is tasked with protecting the righ...      NO  \n",
       "3  Prioritizing the people's welfare is essential...      NO  \n",
       "4  I have fulfilled my duty to my nation by speak...      NO  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_labeled = fea.merge_pairwise_texts(\n",
    "    df1 = df_clause,\n",
    "    df2 = df_llm,\n",
    "    df1_cols = ['sentence_id', 'sentence'],\n",
    "    df2_cols = ['sentence_id_1', 'sentence_id_2', 'verdict']\n",
    ")\n",
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "707ad685",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:26.669807Z",
     "iopub.status.busy": "2026-02-28T00:50:26.669118Z",
     "iopub.status.idle": "2026-02-28T00:50:29.423946Z",
     "shell.execute_reply": "2026-02-28T00:50:29.423265Z"
    },
    "papermill": {
     "duration": 2.767806,
     "end_time": "2026-02-28T00:50:29.425372",
     "exception": false,
     "start_time": "2026-02-28T00:50:26.657566",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set difference: 4,997,977 - 5,952 = 4,997,977 rows\n",
      "✓ Freed unlabeled_pairs (will reload from pickle before finalize)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B0001001p</td>\n",
       "      <td>B0001007p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B0001001p</td>\n",
       "      <td>B0005008p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B0001001p</td>\n",
       "      <td>B0008009p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B0001001p</td>\n",
       "      <td>B0012005p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0001001p</td>\n",
       "      <td>B0019001p</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id1        id2  verdict\n",
       "0  B0001001p  B0001007p      NaN\n",
       "1  B0001001p  B0005008p      NaN\n",
       "2  B0001001p  B0008009p      NaN\n",
       "3  B0001001p  B0012005p      NaN\n",
       "4  B0001001p  B0019001p      NaN"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "if test:\n",
    "    df_predict = fea.merge_pairwise_texts(\n",
    "        df1 = df_clause,\n",
    "        df2 = df_llm_remaining,\n",
    "        df1_cols = ['sentence_id', 'sentence'],\n",
    "        df2_cols = ['sentence_id_1', 'sentence_id_2']\n",
    "    )\n",
    "else:\n",
    "    # At 20M+ pairs, we skip merge_pairwise_texts on the full pool\n",
    "    # (pairs were already validated at generation, text not needed until\n",
    "    #  the small df_final at the end). Just remove already-labeled pairs.\n",
    "    df_predict = fea.setminus(\n",
    "        df_big= unlabeled_pairs,\n",
    "        df_small= df_labeled,\n",
    "        id_cols = ['id1', 'id2']\n",
    "    )\n",
    "    df_predict['verdict'] = np.nan\n",
    "\n",
    "    # Free unlabeled_pairs immediately — it consumed ~3 GB and is no longer\n",
    "    # needed (df_predict is the working copy).  We will reload from pickle\n",
    "    # later when finalize_pipeline_iteration needs it.\n",
    "    del unlabeled_pairs\n",
    "    gc.collect()\n",
    "    print(\"✓ Freed unlabeled_pairs (will reload from pickle before finalize)\")\n",
    "\n",
    "df_predict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d636c53",
   "metadata": {
    "papermill": {
     "duration": 0.008225,
     "end_time": "2026-02-28T00:50:29.441549",
     "exception": false,
     "start_time": "2026-02-28T00:50:29.433324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Embedding All Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecdd312d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:29.458448Z",
     "iopub.status.busy": "2026-02-28T00:50:29.458151Z",
     "iopub.status.idle": "2026-02-28T00:50:29.464675Z",
     "shell.execute_reply": "2026-02-28T00:50:29.463210Z"
    },
    "papermill": {
     "duration": 0.016319,
     "end_time": "2026-02-28T00:50:29.465847",
     "exception": false,
     "start_time": "2026-02-28T00:50:29.449528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Patched transformers.utils.hub\n",
      " - Patched transformers.tokenization_utils_base\n",
      "\n",
      "SUCCESS: The 404 error is now blocked.\n"
     ]
    }
   ],
   "source": [
    "## Patches an error later on with kwargs\n",
    "import transformers.utils.hub\n",
    "import transformers.tokenization_utils_base\n",
    "\n",
    "def _safe_list_templates(*args, **kwargs):\n",
    "    return []\n",
    "\n",
    "transformers.utils.hub.list_repo_templates = _safe_list_templates\n",
    "print(\" - Patched transformers.utils.hub\")\n",
    "\n",
    "# The library had already imported the broken function here, so we must update it.\n",
    "transformers.tokenization_utils_base.list_repo_templates = _safe_list_templates\n",
    "print(\" - Patched transformers.tokenization_utils_base\")\n",
    "\n",
    "print(\"\\nSUCCESS: The 404 error is now blocked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2383f79",
   "metadata": {
    "papermill": {
     "duration": 0.0083,
     "end_time": "2026-02-28T00:50:29.481191",
     "exception": false,
     "start_time": "2026-02-28T00:50:29.472891",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Test and Validation Subsamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ffa5f5a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:29.497947Z",
     "iopub.status.busy": "2026-02-28T00:50:29.497430Z",
     "iopub.status.idle": "2026-02-28T00:50:29.509197Z",
     "shell.execute_reply": "2026-02-28T00:50:29.508371Z"
    },
    "papermill": {
     "duration": 0.022306,
     "end_time": "2026-02-28T00:50:29.510369",
     "exception": false,
     "start_time": "2026-02-28T00:50:29.488063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id1</th>\n",
       "      <th>id2</th>\n",
       "      <th>text1</th>\n",
       "      <th>text2</th>\n",
       "      <th>verdict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>B0228001sc</td>\n",
       "      <td>S0000726003sc</td>\n",
       "      <td>True liberty requires a ruler bound by laws</td>\n",
       "      <td>to uphold accountability in governance</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>S0000726003sc</td>\n",
       "      <td>B0228001sc</td>\n",
       "      <td>to uphold accountability in governance</td>\n",
       "      <td>True liberty requires a ruler bound by laws</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>B0781002p</td>\n",
       "      <td>S0016904011p</td>\n",
       "      <td>The King's authority is limited by the laws an...</td>\n",
       "      <td>Parliamentary involvement ensures that governa...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>S0016904011p</td>\n",
       "      <td>B0781002p</td>\n",
       "      <td>Parliamentary involvement ensures that governa...</td>\n",
       "      <td>The King's authority is limited by the laws an...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>B0217001p</td>\n",
       "      <td>S0022814001p</td>\n",
       "      <td>The right to rule is fundamentally based on co...</td>\n",
       "      <td>The establishment of a constitutional monarchy...</td>\n",
       "      <td>YES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                id1            id2  \\\n",
       "1016     B0228001sc  S0000726003sc   \n",
       "1017  S0000726003sc     B0228001sc   \n",
       "1018      B0781002p   S0016904011p   \n",
       "1019   S0016904011p      B0781002p   \n",
       "1020      B0217001p   S0022814001p   \n",
       "\n",
       "                                                  text1  \\\n",
       "1016        True liberty requires a ruler bound by laws   \n",
       "1017             to uphold accountability in governance   \n",
       "1018  The King's authority is limited by the laws an...   \n",
       "1019  Parliamentary involvement ensures that governa...   \n",
       "1020  The right to rule is fundamentally based on co...   \n",
       "\n",
       "                                                  text2 verdict  \n",
       "1016             to uphold accountability in governance     YES  \n",
       "1017        True liberty requires a ruler bound by laws     YES  \n",
       "1018  Parliamentary involvement ensures that governa...     YES  \n",
       "1019  The King's authority is limited by the laws an...     YES  \n",
       "1020  The establishment of a constitutional monarchy...     YES  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only entailed pairs from sent\n",
    "df_obs_ent = df_labeled.loc[df_labeled['verdict'] == 'YES']\n",
    "df_obs_ent.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8da45922",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-28T00:50:29.527515Z",
     "iopub.status.busy": "2026-02-28T00:50:29.527186Z",
     "iopub.status.idle": "2026-02-28T00:51:59.994936Z",
     "shell.execute_reply": "2026-02-28T00:51:59.993487Z"
    },
    "papermill": {
     "duration": 90.478008,
     "end_time": "2026-02-28T00:51:59.996619",
     "exception": false,
     "start_time": "2026-02-28T00:50:29.518611",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Memory-efficient path for large candidate pools ---\n",
    "# Uses vectorised alpha + equiv_map lookups instead of storing\n",
    "# Python list objects in every row (saves ~8 GB at 75M rows).\n",
    "LARGE_THRESHOLD = 5_000_000\n",
    "\n",
    "if len(df_predict) > LARGE_THRESHOLD:\n",
    "    print(f\"Using memory-efficient path ({len(df_predict):,} candidate rows)\")\n",
    "    df_candidates, df_crossed, equiv_map = fea.prepare_candidates_efficient(\n",
    "        df_obs_ent=df_obs_ent,\n",
    "        df_predict=df_predict,\n",
    "        df_clause=df_clause,\n",
    "    )\n",
    "else:\n",
    "    # Original path (fine for small DataFrames)\n",
    "    df_candidates = fea.add_equivalents_from_pairs(\n",
    "        df3=df_obs_ent,\n",
    "        df4=df_predict,\n",
    "        df3_cols=[\"id1\", \"id2\"],\n",
    "        df4_cols=[\"id1\", \"id2\"],\n",
    "        new_cols=(\"equivalents1\", \"equivalents2\"),\n",
    "        include_self=False,\n",
    "    )\n",
    "    df_candidates = fea.add_alpha_weight_column(\n",
    "        df = df_candidates,\n",
    "        list_col1 = 'equivalents1',\n",
    "        list_col2 = 'equivalents2',\n",
    "        new_col = \"alpha\"\n",
    "    )\n",
    "    equiv_map = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbc10fe",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labeled = fea.add_equivalents_from_pairs(\n",
    "    df3=df_obs_ent,\n",
    "    df4=df_labeled,\n",
    "    df3_cols=[\"id1\", \"id2\"],\n",
    "    df4_cols=[\"id1\", \"id2\"],\n",
    "    new_cols=(\"equivalents1\", \"equivalents2\"),\n",
    "    include_self=False,  # keep the ID itself in the list\n",
    ")\n",
    "\n",
    "df_labeled = fea.add_alpha_weight_column(\n",
    "    df = df_labeled,\n",
    "    list_col1 = 'equivalents1',\n",
    "    list_col2 = 'equivalents2',\n",
    "    new_col = \"alpha\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1178ec2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Equivalence Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713f3b6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only compute crossed pairs here if we used the original (small) path above.\n",
    "# The efficient path already produced df_crossed.\n",
    "if equiv_map is None:\n",
    "    df_crossed = fea.build_equiv_pair_candidates(\n",
    "        df = df_candidates,\n",
    "        id1_col = \"id1\",\n",
    "        id2_col = \"id2\",\n",
    "        equiv1_col = \"equivalents1\",\n",
    "        equiv2_col = \"equivalents2\",\n",
    "    )\n",
    "    df_crossed = fea.merge_pairwise_texts(\n",
    "        df1 = df_clause,\n",
    "        df2 = df_crossed,\n",
    "        df1_cols = ['sentence_id', 'sentence'],\n",
    "        df2_cols = ['id1', 'id2']\n",
    "    )\n",
    "\n",
    "df_crossed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d19e8d1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labeled_crossed = fea.build_equiv_pair_candidates(\n",
    "    df = df_labeled,\n",
    "    id1_col = \"id1\",\n",
    "    id2_col = \"id2\",\n",
    "    equiv1_col = \"equivalents1\",\n",
    "    equiv2_col = \"equivalents2\",\n",
    ")\n",
    "\n",
    "# Retrieve clause sentences\n",
    "df_labeled_crossed = fea.merge_pairwise_texts(\n",
    "    df1 = df_clause,\n",
    "    df2 = df_labeled_crossed,\n",
    "    df1_cols = ['sentence_id', 'sentence'],\n",
    "    df2_cols = ['id1', 'id2']\n",
    ")\n",
    "\n",
    "df_labeled_crossed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6ab831",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Running FEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a77e23",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc, os, pickle\n",
    "\n",
    "temp_dir = \"fea_iterations/temp_data\"\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "n_cand = len(df_candidates)\n",
    "n_obs  = len(df_obs_ent)\n",
    "\n",
    "if n_cand == 0 or n_obs == 0:\n",
    "    print(f\"⚠ Skipping FreeEntailmentAlgorithm (empty data: \"\n",
    "          f\"{n_cand} candidates, {n_obs} entailed pairs)\")\n",
    "    df_final = pd.DataFrame(\n",
    "        columns=['id1', 'id2', 'text1', 'text2', 'entailment_probability'])\n",
    "    fig_html = \"<p>No data for this iteration</p>\"\n",
    "else:\n",
    "    # 1. Pickle all DataFrames to disk for FreeEntailmentAlgorithm\n",
    "    df_candidates.to_pickle(f\"{temp_dir}/df_candidates.pkl\")\n",
    "    print(f\"  Pickled df_candidates: {n_cand:,} rows, cols={list(df_candidates.columns)}\")\n",
    "    df_crossed.to_pickle(f\"{temp_dir}/df_crossed.pkl\")\n",
    "    df_labeled.to_pickle(f\"{temp_dir}/df_labeled.pkl\")\n",
    "    df_labeled_crossed.to_pickle(f\"{temp_dir}/df_labeled_crossed.pkl\")\n",
    "    df_obs_ent.to_pickle(f\"{temp_dir}/df_obs_ent.pkl\")\n",
    "    df_clause.to_pickle(f\"{temp_dir}/df_clause.pkl\")\n",
    "    with open(f\"{temp_dir}/embedding_cache.pkl\", 'wb') as f:\n",
    "        pickle.dump(embedding_cache_finetuned, f)\n",
    "\n",
    "    # 2. FREE all large DataFrames BEFORE spawning FreeEntailmentAlgorithm.\n",
    "    #    papermill runs a NEW kernel process — if we keep these in memory,\n",
    "    #    we'd have two copies of the 75M-row DataFrame across two processes.\n",
    "    try: del df_candidates\n",
    "    except NameError: pass\n",
    "    try: del df_predict\n",
    "    except NameError: pass\n",
    "    try: del df_crossed\n",
    "    except NameError: pass\n",
    "    try: del df_labeled_crossed\n",
    "    except NameError: pass\n",
    "    try: del df_obs_ent\n",
    "    except NameError: pass\n",
    "    try: del equiv_map\n",
    "    except NameError: pass\n",
    "    gc.collect()\n",
    "    print(\"  ✓ Freed large DataFrames before FreeEntailmentAlgorithm subprocess\")\n",
    "\n",
    "    # 3. Execute FreeEntailmentAlgorithm (data already on disk)\n",
    "    df_final, fig_html = fea.run_fea_papermill(\n",
    "        iteration_number=iteration_number,\n",
    "        temp_dir=temp_dir,\n",
    "        data_on_disk=True,\n",
    "    )\n",
    "\n",
    "print(f\"✓ df_final: {len(df_final)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ab8a22",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f28aa4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Task 2: Cleaning LLM Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15f6db4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cap at 100k pairs max — send ALL pairs above threshold (no random subsampling)\n",
    "MAX_LLM_PAIRS = 1000\n",
    "\n",
    "df_final = df_final.reset_index(drop=True)\n",
    "\n",
    "if len(df_final) > MAX_LLM_PAIRS:\n",
    "    df_to_llm = df_final.sample(n=MAX_LLM_PAIRS, random_state=42)\n",
    "    print(f\"Capped df_to_llm at {MAX_LLM_PAIRS:,} (from {len(df_final):,} above threshold)\")\n",
    "else:\n",
    "    df_to_llm = df_final.copy()\n",
    "    print(f\"Sending all {len(df_to_llm):,} pairs above threshold to LLM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c7d020",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_to_llm = fea.format_df_to_llm(df_to_llm, df_clause=df_clause, id_col='sentence_id', text_col='sentence')\n",
    "df_to_llm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d6619d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_to_llm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4473fd4b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Next loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50bf720",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reload unlabeled_pairs from pickle if in production mode.\n",
    "# It was freed earlier to save memory while FreeEntailmentAlgorithm ran.\n",
    "if not test and unlabeled_pairs_path:\n",
    "    import gc\n",
    "    unlabeled_pairs = pd.read_pickle(unlabeled_pairs_path)\n",
    "    print(f\"Reloaded unlabeled_pairs: {len(unlabeled_pairs):,} rows\")\n",
    "    gc.collect()\n",
    "\n",
    "result = fea.finalize_pipeline_iteration(\n",
    "    test=test,\n",
    "    df_to_llm=df_to_llm,\n",
    "    iteration_number=iteration_number,\n",
    "    remaining_llm_calls=remaining_llm_calls,\n",
    "    remaining_llm_calls_path=remaining_llm_calls_path,\n",
    "    unlabeled_pairs=unlabeled_pairs,\n",
    "    unlabeled_pairs_path=unlabeled_pairs_path,\n",
    ")\n",
    "\n",
    "remaining_llm_calls = result['remaining_llm_calls']\n",
    "unlabeled_pairs = result['unlabeled_pairs']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 135.86466,
   "end_time": "2026-02-28T00:52:02.010169",
   "environment_variables": {},
   "exception": null,
   "input_path": "FEA_Pipeline.ipynb",
   "output_path": "fea_iterations\\FEA_Pipeline_iter_2.ipynb",
   "parameters": {
    "budget": 5.0,
    "df_clause_path": "fea_iterations\\loop_data/df_clause.pkl",
    "embedding_cache_path": "fea_iterations\\loop_data/embedding_cache.pkl",
    "input_csv_path": "labeled_pairs/Results_DS_BtoS_iteration_2.csv",
    "iteration_number": 2,
    "remaining_llm_calls_path": null,
    "sent_frac": 0.5,
    "test": false,
    "unlabeled_pairs_path": "fea_iterations\\loop_data/unlabeled_pairs.pkl"
   },
   "start_time": "2026-02-28T00:49:46.145509",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}